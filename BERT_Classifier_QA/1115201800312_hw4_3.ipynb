{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "hw4_3_f.ipynb",
      "provenance": [],
      "toc_visible": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Download TriviaQA"
      ],
      "metadata": {
        "id": "5tqkr3JhB5l4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://nlp.cs.washington.edu/triviaqa/data/triviaqa-rc.tar.gz\n",
        "!tar xf triviaqa-rc.tar.gz"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-13T17:39:26.922252Z",
          "iopub.execute_input": "2022-03-13T17:39:26.922727Z",
          "iopub.status.idle": "2022-03-13T17:43:45.515856Z",
          "shell.execute_reply.started": "2022-03-13T17:39:26.922689Z",
          "shell.execute_reply": "2022-03-13T17:43:45.513759Z"
        },
        "trusted": true,
        "id": "8Jirg3HPB2sw",
        "outputId": "481d2e65-b00e-44d4-b53b-4c453d6bb4cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "--2022-03-13 17:39:27--  http://nlp.cs.washington.edu/triviaqa/data/triviaqa-rc.tar.gz\nResolving nlp.cs.washington.edu (nlp.cs.washington.edu)... 128.208.3.120, 2607:4000:200:12::78\nConnecting to nlp.cs.washington.edu (nlp.cs.washington.edu)|128.208.3.120|:80... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 2665779500 (2.5G) [application/x-gzip]\nSaving to: ‘triviaqa-rc.tar.gz’\n\ntriviaqa-rc.tar.gz  100%[===================>]   2.48G  19.4MB/s    in 2m 35s  \n\n2022-03-13 17:42:02 (16.4 MB/s) - ‘triviaqa-rc.tar.gz’ saved [2665779500/2665779500]\n\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Useful functions for converter"
      ],
      "metadata": {
        "id": "nwUBkqj-B8FM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "\n",
        "def write_json_to_file(json_object, json_file, mode='w', encoding='utf-8'):\n",
        "    with open(json_file, mode, encoding=encoding) as outfile:\n",
        "        json.dump(json_object, outfile, indent=4, sort_keys=True, ensure_ascii=False)\n",
        "\n",
        "\n",
        "def get_file_contents(filename, encoding='utf-8'):\n",
        "    with open(filename, encoding=encoding) as f:\n",
        "        content = f.read()\n",
        "    return content\n",
        "\n",
        "\n",
        "def read_json(filename, encoding='utf-8'):\n",
        "    contents = get_file_contents(filename, encoding=encoding)\n",
        "    return json.loads(contents)\n",
        "\n",
        "\n",
        "def get_file_contents_as_list(file_path, encoding='utf-8', ignore_blanks=True):\n",
        "    contents = get_file_contents(file_path, encoding=encoding)\n",
        "    lines = contents.split('\\n')\n",
        "    lines = [line for line in lines if line != ''] if ignore_blanks else lines\n",
        "    return lines"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "execution": {
          "iopub.status.busy": "2022-03-13T17:44:10.206406Z",
          "iopub.execute_input": "2022-03-13T17:44:10.206690Z",
          "iopub.status.idle": "2022-03-13T17:44:10.216260Z",
          "shell.execute_reply.started": "2022-03-13T17:44:10.206659Z",
          "shell.execute_reply": "2022-03-13T17:44:10.215314Z"
        },
        "trusted": true,
        "id": "4uTYoaH9B2s5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "import nltk\n",
        "import argparse\n",
        "\n",
        "\n",
        "def get_text(qad, domain):\n",
        "    local_file = os.path.join(args.web_dir, qad['Filename']) if domain == 'SearchResults' else os.path.join(args.wikipedia_dir, qad['Filename'])\n",
        "    return get_file_contents(local_file, encoding='utf-8')\n",
        "\n",
        "\n",
        "def select_relevant_portion(text):\n",
        "    paras = text.split('\\n')\n",
        "    selected = []\n",
        "    done = False\n",
        "    for para in paras:\n",
        "        sents = sent_tokenize.tokenize(para)\n",
        "        for sent in sents:\n",
        "            words = nltk.word_tokenize(sent)\n",
        "            for word in words:\n",
        "                selected.append(word)\n",
        "                if len(selected) >= args.max_num_tokens:\n",
        "                    done = True\n",
        "                    break\n",
        "            if done:\n",
        "                break\n",
        "        if done:\n",
        "            break\n",
        "        selected.append('\\n')\n",
        "    st = ' '.join(selected).strip()\n",
        "    return st\n",
        "\n",
        "\n",
        "def add_triple_data(datum, page, domain):\n",
        "    qad = {'Source': domain}\n",
        "    for key in ['QuestionId', 'Question', 'Answer']:\n",
        "        qad[key] = datum[key]\n",
        "    for key in page:\n",
        "        qad[key] = page[key]\n",
        "    return qad\n",
        "\n",
        "\n",
        "def get_qad_triples(data):\n",
        "    qad_triples = []\n",
        "    for datum in data['Data']:\n",
        "        for key in ['EntityPages', 'SearchResults']:\n",
        "            for page in datum.get(key, []):\n",
        "                qad = add_triple_data(datum, page, key)\n",
        "                qad_triples.append(qad)\n",
        "    return qad_triples\n",
        "\n",
        "\n",
        "def convert_to_squad_format(qa_json_file, squad_file):\n",
        "    qa_json = read_triviaqa_data(qa_json_file)\n",
        "    qad_triples = get_qad_triples(qa_json)\n",
        "\n",
        "    random.seed(args.seed)\n",
        "    random.shuffle(qad_triples)\n",
        "\n",
        "    data = []\n",
        "    for qad in tqdm(qad_triples):\n",
        "        qid = qad['QuestionId']\n",
        "\n",
        "        text = get_text(qad, qad['Source'])\n",
        "        selected_text = select_relevant_portion(text)\n",
        "\n",
        "        question = qad['Question']\n",
        "        para = {'context': selected_text, 'qas': [{'question': question, 'answers': []}]}\n",
        "        data.append({'paragraphs': [para]})\n",
        "        qa = para['qas'][0]\n",
        "        qa['id'] = get_question_doc_string(qid, qad['Filename'])\n",
        "        qa['qid'] = qid\n",
        "\n",
        "        ans_string, index = answer_index_in_document(qad['Answer'], selected_text)\n",
        "        if index == -1:\n",
        "            if qa_json['Split'] == 'train':\n",
        "                continue\n",
        "        else:\n",
        "            qa['answers'].append({'text': ans_string, 'answer_start': index})\n",
        "\n",
        "        if qa_json['Split'] == 'train' and len(data) >= args.sample_size and qa_json['Domain'] == 'Web':\n",
        "            break\n",
        "\n",
        "    squad = {'data': data, 'version': qa_json['Version']}\n",
        "    write_json_to_file(squad, squad_file)\n",
        "    print ('Added', len(data))\n",
        "\n",
        "\n",
        "def get_args():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--triviaqa_file', help='Triviaqa file')\n",
        "    parser.add_argument('--squad_file', help='Squad file')\n",
        "    parser.add_argument('--wikipedia_dir', help='Wikipedia doc dir')\n",
        "    parser.add_argument('--web_dir', help='Web doc dir')\n",
        "\n",
        "    parser.add_argument('--seed', default=10, type=int, help='Random seed')\n",
        "    parser.add_argument('--max_num_tokens', default=800, type=int, help='Maximum number of tokens from a document')\n",
        "    parser.add_argument('--sample_size', default=80000, type=int, help='Random seed')\n",
        "    parser.add_argument('--tokenizer', default='tokenizers/punkt/english.pickle', help='Sentence tokenizer')\n",
        "    args = parser.parse_args()\n",
        "    return args"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-13T17:44:13.120415Z",
          "iopub.execute_input": "2022-03-13T17:44:13.120669Z",
          "iopub.status.idle": "2022-03-13T17:44:14.727663Z",
          "shell.execute_reply.started": "2022-03-13T17:44:13.120639Z",
          "shell.execute_reply": "2022-03-13T17:44:14.726945Z"
        },
        "trusted": true,
        "id": "-djaouX4B2s6",
        "outputId": "10f32333-7669-48d2-86b7-a1bcb397842b"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_key_to_ground_truth(data):\n",
        "    if data['Domain'] == 'Wikipedia':\n",
        "        return {datum['QuestionId']: datum['Answer'] for datum in data['Data']}\n",
        "    else:\n",
        "        return get_qd_to_answer(data)\n",
        "\n",
        "\n",
        "def get_question_doc_string(qid, doc_name):\n",
        "    return '{}--{}'.format(qid, doc_name)\n",
        "\n",
        "def get_qd_to_answer(data):\n",
        "    key_to_answer = {}\n",
        "    for datum in data['Data']:\n",
        "        for page in datum.get('EntityPages', []) + datum.get('SearchResults', []):\n",
        "            qd_tuple = get_question_doc_string(datum['QuestionId'], page['Filename'])\n",
        "            key_to_answer[qd_tuple] = datum['Answer']\n",
        "    return key_to_answer\n",
        "\n",
        "\n",
        "def read_clean_part(datum):\n",
        "    for key in ['EntityPages', 'SearchResults']:\n",
        "        new_page_list = []\n",
        "        for page in datum.get(key, []):\n",
        "            if page['DocPartOfVerifiedEval']:\n",
        "                new_page_list.append(page)\n",
        "        datum[key] = new_page_list\n",
        "    assert len(datum['EntityPages']) + len(datum['SearchResults']) > 0\n",
        "    return datum\n",
        "\n",
        "\n",
        "def read_triviaqa_data(qajson):\n",
        "    data = read_json(qajson)\n",
        "    # read only documents and questions that are a part of clean data set\n",
        "    if data['VerifiedEval']:\n",
        "        clean_data = []\n",
        "        for datum in data['Data']:\n",
        "            if datum['QuestionPartOfVerifiedEval']:\n",
        "                if data['Domain'] == 'Web':\n",
        "                    datum = read_clean_part(datum)\n",
        "                clean_data.append(datum)\n",
        "        data['Data'] = clean_data\n",
        "    return data\n",
        "\n",
        "\n",
        "def answer_index_in_document(answer, document):\n",
        "    answer_list = answer['NormalizedAliases']\n",
        "    for answer_string_in_doc in answer_list:\n",
        "        index = document.lower().find(answer_string_in_doc)\n",
        "        if index != -1:\n",
        "            return answer_string_in_doc, index\n",
        "    return answer['NormalizedValue'], -1"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-13T17:44:35.715461Z",
          "iopub.execute_input": "2022-03-13T17:44:35.715753Z",
          "iopub.status.idle": "2022-03-13T17:44:35.728080Z",
          "shell.execute_reply.started": "2022-03-13T17:44:35.715724Z",
          "shell.execute_reply": "2022-03-13T17:44:35.727288Z"
        },
        "trusted": true,
        "id": "rZMhB3JxB2s9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import argparse\n",
        "import json\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "\n",
        "def answer_index_in_document(answer, document):\n",
        "    answer_list = answer['Aliases'] + answer['NormalizedAliases']\n",
        "    for answer_string_in_doc in answer_list:\n",
        "        index = document.find(answer_string_in_doc)\n",
        "        if index != -1:\n",
        "            return answer_string_in_doc, index\n",
        "    return answer['NormalizedValue'], -1\n",
        "\n",
        "\n",
        "def select_relevant_portion(text):\n",
        "    paras = text.split('\\n')\n",
        "    selected = []\n",
        "    done = False\n",
        "    for para in paras:\n",
        "        sents = sent_tokenize.tokenize(para)\n",
        "        for sent in sents:\n",
        "            words = nltk.word_tokenize(sent)\n",
        "            for word in words:\n",
        "                selected.append(word)\n",
        "                if len(selected) >= 800:\n",
        "                    done = True\n",
        "                    break\n",
        "            if done:\n",
        "                break\n",
        "        if done:\n",
        "            break\n",
        "        selected.append('\\n')\n",
        "    st = ' '.join(selected).strip()\n",
        "    return st\n",
        "\n",
        "\n",
        "def triviaqa_to_squad_format(triviaqa_file, data_dir, output_file):\n",
        "    triviaqa_json = read_triviaqa_data(triviaqa_file)\n",
        "    qad_triples = get_qad_triples(triviaqa_json)\n",
        "\n",
        "    data = []\n",
        "\n",
        "    for triviaqa_example in qad_triples:\n",
        "        question_text = triviaqa_example['Question']\n",
        "        text = get_file_contents(os.path.join(data_dir, triviaqa_example['Filename']), encoding='utf-8')\n",
        "        context = select_relevant_portion(text)\n",
        "\n",
        "        para = {'context': context, 'qas': [{'question': question_text, 'answers': []}]}\n",
        "        data.append({'paragraphs': [para]})\n",
        "        qa = para['qas'][0]\n",
        "        qa['id'] = get_question_doc_string(triviaqa_example['QuestionId'], triviaqa_example['Filename'])\n",
        "        qa['is_impossible'] = True\n",
        "        ans_string, index = answer_index_in_document(triviaqa_example['Answer'], context)\n",
        "\n",
        "        if index != -1:\n",
        "            qa['answers'].append({'text': ans_string, 'answer_start': index})\n",
        "            qa['is_impossible'] = False\n",
        "\n",
        "    triviaqa_as_squad = {'data': data, 'version': '2.0'}\n",
        "\n",
        "    with open(output_file, 'w', encoding='utf-8') as outfile:\n",
        "        json.dump(triviaqa_as_squad, outfile, indent=2, sort_keys=True, ensure_ascii=False)\n",
        "\n",
        "\n",
        "\n",
        "sent_tokenize = nltk.data.load('tokenizers/punkt/english.pickle')\n",
        "\n",
        "triviaqa_to_squad_format(\"qa/wikipedia-train.json\", \"evidence/wikipedia/\", \"triviaqa_train.json\")\n",
        "\n",
        "triviaqa_to_squad_format(\"qa/wikipedia-dev.json\", \"evidence/wikipedia/\", \"triviaqa_dev.json\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-13T17:44:39.291072Z",
          "iopub.execute_input": "2022-03-13T17:44:39.291387Z",
          "iopub.status.idle": "2022-03-13T18:06:30.419643Z",
          "shell.execute_reply.started": "2022-03-13T17:44:39.291353Z",
          "shell.execute_reply": "2022-03-13T18:06:30.418913Z"
        },
        "trusted": true,
        "id": "I5_YZ1XiB2s_",
        "outputId": "4c1245ba-c67e-4940-f78b-1f22c9ee40e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports, Downloads & Cuda availability | MUST-RUN"
      ],
      "metadata": {
        "id": "4M2jfi7xCAIe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "from transformers import BertTokenizer\n",
        "from torch.utils.data import TensorDataset\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "import random\n",
        "import numpy as np\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from transformers import AutoTokenizer,AdamW,BertForQuestionAnswering,DistilBertForQuestionAnswering\n",
        "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
        "\n",
        "import string\n",
        "import nltk\n",
        "import unicodedata\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "import re\n",
        "from itertools import cycle\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import time\n",
        "import datetime\n",
        "\n",
        "\n",
        "if torch.cuda.is_available():    \n",
        "    device = torch.device(\"cuda\")\n",
        "    print('Using gpu')\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print('Using cpu')\n",
        "\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "!mkdir squad\n",
        "!wget https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json -O squad/train-v2.0.json\n",
        "!wget https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json -O squad/dev-v2.0.json"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-13T18:27:48.842757Z",
          "iopub.execute_input": "2022-03-13T18:27:48.843559Z",
          "iopub.status.idle": "2022-03-13T18:28:07.263625Z",
          "shell.execute_reply.started": "2022-03-13T18:27:48.843462Z",
          "shell.execute_reply": "2022-03-13T18:28:07.262775Z"
        },
        "trusted": true,
        "id": "pSzzU_DAB2tA",
        "outputId": "8c7df765-1d9a-49dc-8772-ee168d99a58a"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Requirement already satisfied: transformers in /opt/conda/lib/python3.7/site-packages (4.15.0)\nRequirement already satisfied: sacremoses in /opt/conda/lib/python3.7/site-packages (from transformers) (0.0.47)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers) (4.11.2)\nRequirement already satisfied: tokenizers<0.11,>=0.10.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.10.3)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (21.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers) (3.4.2)\nRequirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.2.1)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (6.0)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers) (4.62.3)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (2021.11.10)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers) (2.26.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (1.20.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->transformers) (3.0.6)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.6.0)\nRequirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2.0.9)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (1.26.7)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (3.1)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2021.10.8)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (1.16.0)\nRequirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (8.0.3)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (1.1.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\nUsing gpu\nmkdir: cannot create directory ‘squad’: File exists\n--2022-03-13 18:28:05--  https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json\nResolving rajpurkar.github.io (rajpurkar.github.io)... 185.199.111.153, 185.199.110.153, 185.199.108.153, ...\nConnecting to rajpurkar.github.io (rajpurkar.github.io)|185.199.111.153|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 42123633 (40M) [application/json]\nSaving to: ‘squad/train-v2.0.json’\n\nsquad/train-v2.0.js 100%[===================>]  40.17M   219MB/s    in 0.2s    \n\n2022-03-13 18:28:06 (219 MB/s) - ‘squad/train-v2.0.json’ saved [42123633/42123633]\n\n--2022-03-13 18:28:07--  https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json\nResolving rajpurkar.github.io (rajpurkar.github.io)... 185.199.111.153, 185.199.108.153, 185.199.109.153, ...\nConnecting to rajpurkar.github.io (rajpurkar.github.io)|185.199.111.153|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 4370528 (4.2M) [application/json]\nSaving to: ‘squad/dev-v2.0.json’\n\nsquad/dev-v2.0.json 100%[===================>]   4.17M  --.-KB/s    in 0.03s   \n\n2022-03-13 18:28:07 (128 MB/s) - ‘squad/dev-v2.0.json’ saved [4370528/4370528]\n\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing & Print functions"
      ],
      "metadata": {
        "id": "f6_l8oWVCIUH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_punctuation(text):\n",
        "    punctuationfree=\"\".join([i for i in text if (i not in string.punctuation)])\n",
        "    return punctuationfree\n",
        "\n",
        "def add_prefix_NOT_(text):\n",
        "  neg_array = [\"n’t\",\"n't\", \"not\", \"no\", \"never\"]\n",
        "  final_txt = \"\"\n",
        "  flg = 0;\n",
        "  for i in text.split():\n",
        "    if flg == 1:\n",
        "      final_txt = final_txt + \" NOT_\" + i\n",
        "    else:\n",
        "      final_txt = final_txt + \" \" + i\n",
        "    if i.endswith(tuple(neg_array)):\n",
        "      flg = 1\n",
        "    else:\n",
        "      flg = 0\n",
        "  return final_txt\n",
        "\n",
        "def strip_accents(s):\n",
        "   return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
        "                  if unicodedata.category(c) != 'Mn')\n",
        "   \n",
        "def remove_emoji(string):\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                               u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
        "                               u\"\\U00002702-\\U000027B0\"\n",
        "                               u\"\\U00002702-\\U000027B0\"\n",
        "                               u\"\\U000024C2-\\U0001F251\"\n",
        "                               u\"\\U0001f926-\\U0001f937\"\n",
        "                               u\"\\U00010000-\\U0010ffff\"\n",
        "                               u\"\\u2640-\\u2642\"\n",
        "                               u\"\\u2600-\\u2B55\"\n",
        "                               u\"\\u200d\"\n",
        "                               u\"\\u23cf\"\n",
        "                               u\"\\u23e9\"\n",
        "                               u\"\\u231a\"\n",
        "                               u\"\\ufe0f\"  # dingbats\n",
        "                               u\"\\u3030\"\n",
        "                               \"]+\", flags=re.UNICODE)\n",
        "    return emoji_pattern.sub(r'', string)\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "def stem(text):\n",
        "    return [stemmer.stem(word) for word in text]\n",
        "def lem(text):\n",
        "  return [lemmatizer.lemmatize(word) for word in text]\n",
        "def split(text):\n",
        "  return re.split(' ',text)\n",
        "def stringify(text):\n",
        "  return ' '.join(text)\n",
        "\n",
        "class Dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings):\n",
        "        self.encodings = encodings\n",
        "\n",
        "    def __getitem__(self, id):\n",
        "        return {key: torch.tensor(val[id]) for key, val in self.encodings.items()}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.encodings.input_ids)\n",
        "\n",
        "def compute_f1(prediction, truth):\n",
        "    pred_tokens = normalize_text(prediction).split()\n",
        "    truth_tokens = normalize_text(truth).split()\n",
        "\n",
        "    # if either the prediction or the truth is no-answer then f1 = 1 if they agree, 0 otherwise\n",
        "    if len(pred_tokens) == 0 or len(truth_tokens) == 0:\n",
        "        return int(pred_tokens == truth_tokens)\n",
        "\n",
        "    common_tokens = set(pred_tokens) & set(truth_tokens)\n",
        "\n",
        "    # if there are no common tokens then f1 = 0\n",
        "    if len(common_tokens) == 0:\n",
        "        return 0\n",
        "\n",
        "    prec = len(common_tokens) / len(pred_tokens)\n",
        "    rec = len(common_tokens) / len(truth_tokens)\n",
        "\n",
        "    return 2 * (prec * rec) / (prec + rec)\n",
        "\n",
        "def normalize_text(s):\n",
        "    import string, re\n",
        "\n",
        "    def remove_articles(text):\n",
        "        regex = re.compile(r\"\\b(a|an|the)\\b\", re.UNICODE)\n",
        "        return re.sub(regex, \" \", text)\n",
        "\n",
        "    def white_space_fix(text):\n",
        "        return \" \".join(text.split())\n",
        "\n",
        "    def remove_punc(text):\n",
        "        exclude = set(string.punctuation)\n",
        "        return \"\".join(ch for ch in text if ch not in exclude)\n",
        "\n",
        "    def lower(text):\n",
        "        return text.lower()\n",
        "\n",
        "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
        "\n",
        "  \n",
        "def print_function(iters,losses,losses_Val):\n",
        "  \n",
        "  plt.title(\"Learning Curve \")\n",
        "  plt.plot(iters, losses, label=\"Train\")\n",
        "  plt.plot(iters, losses_Val, label=\"Val\")\n",
        "  plt.xlabel(\"Epoch\")\n",
        "  plt.ylabel(\"Loss\")\n",
        "  plt.legend(loc='best')\n",
        "  plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-13T18:28:07.267135Z",
          "iopub.execute_input": "2022-03-13T18:28:07.267360Z",
          "iopub.status.idle": "2022-03-13T18:28:07.290011Z",
          "shell.execute_reply.started": "2022-03-13T18:28:07.267333Z",
          "shell.execute_reply": "2022-03-13T18:28:07.289234Z"
        },
        "trusted": true,
        "id": "IkmxzNo5B2tB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Read train & validation files"
      ],
      "metadata": {
        "id": "5f0Fto-xCKyk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path_train = Path('triviaqa_train.json')\n",
        "path_val = Path('triviaqa_dev.json')\n",
        "path_val_sq = Path('squad/dev-v2.0.json')\n",
        "with open(path_train, 'rb') as f:\n",
        "    dictionary_train = json.load(f)\n",
        "    \n",
        "con_train, quest_train, ans_train = [],[],[]\n",
        "\n",
        "for data in dictionary_train['data']:\n",
        "    for paragraphs in data['paragraphs']:\n",
        "        context = paragraphs['context']\n",
        "        for qas in paragraphs['qas']:\n",
        "            question = qas['question']\n",
        "            for answers in qas['answers']:\n",
        "                con_train.append(context)\n",
        "                quest_train.append(question)\n",
        "                ans_train.append(answers)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-13T18:28:07.291249Z",
          "iopub.execute_input": "2022-03-13T18:28:07.291584Z",
          "iopub.status.idle": "2022-03-13T18:28:13.287734Z",
          "shell.execute_reply.started": "2022-03-13T18:28:07.291545Z",
          "shell.execute_reply": "2022-03-13T18:28:13.286920Z"
        },
        "trusted": true,
        "id": "AwzLPgv3B2tD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine tuning DistilBert for qa in TriviaQA w/ lr=2e-5 eps=5e-9"
      ],
      "metadata": {
        "id": "CGq-zVylCNlo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()\n",
        "for answer in ans_train:\n",
        "    answer['answer_end'] = answer['answer_start'] + len(answer['text'])\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "\n",
        "enc_train = tokenizer(con_train, quest_train, truncation=True,max_length = 350, padding='max_length')\n",
        "\n",
        "spos_list = []\n",
        "epos_list = []\n",
        "\n",
        "for i in range(len(ans_train)):\n",
        "  spos_list.append(enc_train.char_to_token(i, ans_train[i]['answer_start']))\n",
        "  epos_list.append(enc_train.char_to_token(i, ans_train[i]['answer_end']))\n",
        "  if spos_list[-1] is None:\n",
        "    spos_list[-1] = tokenizer.model_max_length\n",
        "  if epos_list[-1] is None:\n",
        "    epos_list[-1] = tokenizer.model_max_length\n",
        "\n",
        "enc_train.update({'start_positions': spos_list, 'end_positions': epos_list})\n",
        "\n",
        "train_dataset = Dataset(enc_train)\n",
        "\n",
        "#Batch size & dataloaders for training loop\n",
        "batch_size = 4\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "\n",
        "#Using bert-base-uncased BertForSequenceClassification for our model\n",
        "model = DistilBertForQuestionAnswering.from_pretrained('distilbert-base-uncased').to(device)\n",
        "\n",
        "optimizer = AdamW(model.parameters(),lr = 2e-5,eps = 5e-9)\n",
        "\n",
        "#For Bert fine tuning we will need to train the pre-trained model for our dataset\n",
        "epochs = 3\n",
        "\n",
        "sch_steps = len(train_dataloader) * epochs\n",
        "\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = 0,num_training_steps = sch_steps)\n",
        "\n",
        "losses = []\n",
        "losses_Val = []\n",
        "\n",
        "n_epoch=0\n",
        "iters = []\n",
        "\n",
        "for curr_epoch in range(0, epochs):\n",
        "    batch_lo = []\n",
        "    val_batch_los = []\n",
        "    \n",
        "\n",
        "    print(f\"\\nEpoch: {curr_epoch}\")\n",
        "    print(\"Training:\")\n",
        "    model.train()\n",
        "\n",
        "    for batch in tqdm(train_dataloader):\n",
        "\n",
        "        #Get batche's data\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        start_positions = batch['start_positions'].to(device)\n",
        "        end_positions = batch['end_positions'].to(device)\n",
        "\n",
        "        #Clearning gradients\n",
        "        model.zero_grad()\n",
        "\n",
        "        #Run a forward pass\n",
        "        outputs = model(input_ids, attention_mask=attention_mask, start_positions=start_positions, end_positions=end_positions)\n",
        "\n",
        "        #Get loss and logits data\n",
        "        loss = outputs[0]\n",
        "        # do a backwards pass \n",
        "        loss.backward()\n",
        "\n",
        "        #Clipping that helps for exploding gradients problem\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        batch_lo.append(loss.item())\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "\n",
        "    losses.append(sum(batch_lo)/len(train_dataloader))\n",
        "\n",
        "    print(f\" Total Average loss : {sum(batch_lo)/len(train_dataloader)}\")\n",
        "  \n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-13T18:28:13.289796Z",
          "iopub.execute_input": "2022-03-13T18:28:13.290088Z",
          "iopub.status.idle": "2022-03-13T20:00:00.788349Z",
          "shell.execute_reply.started": "2022-03-13T18:28:13.290050Z",
          "shell.execute_reply": "2022-03-13T20:00:00.787545Z"
        },
        "trusted": true,
        "id": "H6esfX5FB2tE",
        "outputId": "ebf7bac6-7417-42c8-9401-16eadaa3bec9"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForQuestionAnswering: ['vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_transform.bias']\n- This IS expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nEpoch: 0\nTraining:\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 19278/19278 [29:31<00:00, 10.88it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": " Total Average loss : 1.425250962463926\n\nEpoch: 1\nTraining:\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 19278/19278 [29:30<00:00, 10.89it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": " Total Average loss : 0.8337224429889346\n\nEpoch: 2\nTraining:\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 19278/19278 [29:25<00:00, 10.92it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": " Total Average loss : 0.5417537616457181\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Validation in TriviaQA"
      ],
      "metadata": {
        "id": "c6SvOFFLCVWW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(path_val, 'rb') as f:\n",
        "    data = json.load(f)\n",
        "    \n",
        "model.eval()\n",
        "f1_list = []\n",
        "iters = 0\n",
        "\n",
        "for group in tqdm(data['data']):\n",
        "    for passage in group['paragraphs']:\n",
        "        context = passage['context']\n",
        "        for qa in passage['qas']:\n",
        "            question = qa['question']\n",
        "            for answer in qa['answers']:\n",
        "                enc = tokenizer.encode_plus(context,question,truncation=True, return_tensors = 'pt')\n",
        "                enc.to(device)\n",
        "                outputs = model(**enc)\n",
        "                start = torch.argmax(outputs.start_logits)\n",
        "                end = torch.argmax(outputs.end_logits) + 1\n",
        "                answer_pred = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(enc['input_ids'][0][start:end]))\n",
        "                f1 = compute_f1(answer_pred,answer['text'])\n",
        "                f1_list.append(f1)\n",
        "                iters+=1\n",
        "                    \n",
        "\n",
        "print(sum(f1_list)/iters)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-13T20:00:05.522583Z",
          "iopub.execute_input": "2022-03-13T20:00:05.523149Z",
          "iopub.status.idle": "2022-03-13T20:02:34.701174Z",
          "shell.execute_reply.started": "2022-03-13T20:00:05.523110Z",
          "shell.execute_reply": "2022-03-13T20:02:34.700318Z"
        },
        "trusted": true,
        "id": "cxPf8VIMB2tG",
        "outputId": "412be7e2-e5ed-41fb-e012-1810b6c1da62"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "100%|██████████| 14229/14229 [02:28<00:00, 95.92it/s] ",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "0.23284294569437827\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Validation in SQuAD 2.0"
      ],
      "metadata": {
        "id": "2RpGbkJtCXnH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(path_val_sq, 'rb') as f:\n",
        "    data = json.load(f)\n",
        "    \n",
        "model.eval()\n",
        "f1_list = []\n",
        "iters = 0\n",
        "\n",
        "for group in tqdm(data['data']):\n",
        "    for passage in group['paragraphs']:\n",
        "        context = passage['context']\n",
        "        for qa in passage['qas']:\n",
        "            question = qa['question']\n",
        "            for answer in qa['answers']:\n",
        "                enc = tokenizer.encode_plus(context,question,truncation=True, return_tensors = 'pt')\n",
        "                enc.to(device)\n",
        "                outputs = model(**enc)\n",
        "                start = torch.argmax(outputs.start_logits)\n",
        "                end = torch.argmax(outputs.end_logits) + 1\n",
        "                answer_pred = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(enc['input_ids'][0][start:end]))\n",
        "                f1 = compute_f1(answer_pred,answer['text'])\n",
        "                f1_list.append(f1)\n",
        "                iters+=1\n",
        "                    \n",
        "\n",
        "print(sum(f1_list)/iters)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-13T20:02:50.072113Z",
          "iopub.execute_input": "2022-03-13T20:02:50.072431Z",
          "iopub.status.idle": "2022-03-13T20:05:14.835512Z",
          "shell.execute_reply.started": "2022-03-13T20:02:50.072395Z",
          "shell.execute_reply": "2022-03-13T20:05:14.834766Z"
        },
        "trusted": true,
        "id": "1JZlyl_rB2tH",
        "outputId": "d9cf822c-64a1-455c-cf58-dba4a3c044c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "100%|██████████| 35/35 [02:24<00:00,  4.13s/it]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "0.18114353810558886\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        }
      ]
    }
  ]
}